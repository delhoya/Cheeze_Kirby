# 새로 익히기 개요

### 컴퓨터 비전
		
#### Segmentation & Clustering
+ Morphological operation
	+ Erosion, Dilation, Opening, Closing 

+ Clustering as segmentation
	+ K-means, Mean-shift

+ Graph based clustering
	+ Graph cut, spectral clustering

+ Fitting
	+ Deformable(Active) contours

#### Classification
+ Bag of Features
	+ https://darkpgmr.tistory.com/125 
	+ 기하학적인 관계 (homography 매칭) vs Bag of Words (히스토그램 매칭)
	+ Extract Features (e.g., SIFT) from Images + Learn Visual Dictionary (e.g., K-means clustering)
	+ Encode: Bags-of-Words (BOW) vectors for each image
	+ Classify: Train and test data using BOW vectors
	
+ Classifier
	+ K-Nearest Neighbor
	+ Naïve Bayes
	+ Support Vector Machine

#### Classification & Network
+ Convolution Networks 
+ Neural Networks
+ Convolutional Neural Networks 

#### Stereo Vision
+ Two-view Geometry
	+ Epipolar Geometry
	+ Essential matrix
	+ Fundamental matrix

+ Stereo Vision
	+ Stereo Rectification
	+ Stereo Matching
	+ Disparity & Depth

#### Motion Tracking
+ Motion
+ Optical Flow   
	+ Lucas-Kanade
	+ Horn-Schunck
+ Visual Tracking
	+ Kanade-Lucas-Tomasi
	+ Mean shift tracking
	+ Cam shift tracking

#### Robot Vision
+ CV vs. MV vs. RV
+ Visual Servoing
+ Visual SLAM


### 자연어처리 


#### POS Tagging (with HMM)
+ Basic Probability Theory / Estimating Probability

+ POS Tagging 
	+ Language Model(1) - Bayesian Theorem 부터 Naive Bayes까지
	+ https://www.quantumdl.com/entry/Language-Model-Naive-Bayes?category=691797 
	+ Language Model(2) - HMM을 이용한 Part of Speech(POS) Tagging
	+ https://www.quantumdl.com/entry/Language-Model2-HMM%EC%9D%84-%EC%9D%B4%EC%9A%A9%ED%95%9C-Part-of-SpeechPOS-Tagging

+ HMM & Viterbi 
	+ [Algorithms for NLP] POS tagging, HMM, Viterbi
	+ https://m.blog.naver.com/ckdgus1433/221812095900 
	+ Baum–Welch algorithm
	+ https://hyunlee103.tistory.com/53?category=999732

#### 워드임베딩 (Word Embedding)

+ Basic idea of learning neural network word embeddings 
+ Approaches for Word Embedding
	+ Word2Vec 
	+ Word2Vec (1) : 단어 임베딩 & CBOW 모델
	+ https://reniew.github.io/21/
	+ Word2Vec (2) : Skip Gram 모델 & 튜닝 기법
	+ https://reniew.github.io/22/ 	
	+ Glove  
	+ https://ratsgo.github.io/from%20frequency%20to%20semantics/2017/04/09/glove/
	+ Ranking-based
	+ [논문 요약 2018-07] Improving Negative Sampling for Word Representation using Self-embedded Features
	+ https://m.blog.naver.com/PostView.naver?isHttpsRedirect=true&blogId=hist0134&logNo=221218233489

#### 딥러닝 기반의 자연어처리 모델 (RNN)
+ Convolutional Neural Networks

+ Recurrent Neural Networks (LSTM,GRU)
	+ Gated Recurrent Units
	+ Long Short-Term Memory (이해하기) 
	+ https://dgkim5360.tistory.com/entry/understanding-long-short-term-memory-lstm-kr?category=912687
	
	
#### 딥러닝 기반의 자연어처리 모델 (Attention)
+ Attention Mechanism
	+ 먼 거리에 위치한 단어의 가중치를 현재 단어에 효과적으로 반영할 수 있는 방법이 없을까? 
	+ Attention 구조는 각 입력이 출력 상태에 연결 신경망 추가 (모든 단어들의 가중치)
	+ Transformer - Harder, Better, Faster, Stronger
	+ https://blog.pingpong.us/transformer-review/


+ 더 나아간 모델들?
+ Introduction to Pre-trained Language Representations
	+ (Feature-based approach) ELMo
	+ (Fine-tuning approach) OpenAI GPT
	+ (Fine-tuning approach) BERT
	+ Various Types of BERT (RoBerta, Albert) 
	+ T5
		+ Exploring Transfer Learning with T5 : the Text-To-Text Transfer Transformer (1)
		+ Exploring Transfer Learning with T5 : the Text-To-Text Transfer Transformer (2)
		+	https://soundprovider.tistory.com/entry/Exploring-Transfer-Learning-with-T5-the-Text-To-Text-Transfer-Transformer-2?category=1126750

### 자연어처리 Pipeline

#### NLP PipeLine
+ NLP의 작업 과정 
+ https://www.quantumdl.com/entry/NLP%EC%9D%98-%EC%9E%91%EC%97%85-%EA%B3%BC%EC%A0%95?category=691797 


#### 워드 임베딩 (Feature Extraction) 
+ pass

#### 언어 모델 : 다음 문장이 적합한지 컴퓨터가 확률을 계산하는 것

+ https://wikidocs.net/21668 
+ 통계모형 
	+	https://wikidocs.net/21687 
+ 인공 신경망 모형
	+ https://wikidocs.net/46496

+ Train Model : 

+ LSTM , GRU 

+ Attention Model : 
+ BERT
	+ https://simonezz.tistory.com/48?category=892980 

+ OpenAI GPT-1	
	+  https://simonezz.tistory.com/73?category=892980

+ ELMO (Pre_train_model)
	+ https://wikidocs.net/33793


### 머신러닝

+ Genetic Algorithm

+ NN
	+ from keras.datasets import mnist 
	+ from keras.datasets import imdb
	+ from keras.datasets import reuters
	+ from keras.datasets import boston_housing 

# 파이썬

+ 내어쓰기 / 들여쓰기 (Shift + TAB  / TAB)



# 새로 찾기 

### 데이터셋 선정

+ GAN 
	+ 데이터셋 구축에서 GAN의 중요성  
	+ https://blog.testworks.co.kr/importance_of_gan_in_ai_dataset_building/ 

### 모델 선정


#### 모델 분류

+ Generative Model / Discriminative Model
	+ https://m.blog.naver.com/PostView.naver?isHttpsRedirect=true&blogId=ehdrndd&logNo=221520140545


#### 생성모델 

+ 생성모델이란 무엇일까? 
	+  https://minsuksung-ai.tistory.com/12

+ AutoEncoder (1) : Maximum likelihood 관점에서의 해석
	+ https://junstar92.tistory.com/156?category=905977

#### 강화학습

+ 강화학습이란 무엇일까?
	+ https://minsuksung-ai.tistory.com/13 


### 최적화 

+ 최적화 기법의 직관적 이해
	+ https://darkpgmr.tistory.com/149
+ 함수최적화 기법 정리
	+ https://darkpgmr.tistory.com/142
 
### 추론

+ 딥러닝 모델 서비스 A-Z 1편 - 연산 최적화 및 모델 경량화
	+ https://blog.pingpong.us/ml-model-optimize/


### 챌린지

+ MATLAB 대학생 AI 챌린지 2021
+ https://kr.mathworks.com/academia/student-challenge/2021/ai-challenge/winners.html?s_v1=41082&elqem=3570651_EM_KR_DIR_21-11_MOE-EDU&elqTrackId=4480e0b764164201a8c307d84d6af6a5&elq=ce9ed9c45b9d4cd79dcd36abc26bd796&elqaid=41082&elqat=1&elqCampaignId=15124
