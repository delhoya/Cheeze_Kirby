# 새로 익히기 개요

### 컴퓨터 비전
		
#### Segmentation & Clustering
+ Morphological operation
	+ Erosion, Dilation, Opening, Closing 

+ Clustering as segmentation
	+ K-means, Mean-shift

+ Graph based clustering
	+ Graph cut, spectral clustering

+ Fitting
	+ Deformable(Active) contours

#### Classification
+ Bag of Features
	+ 기하학적인 관계 (homography 매칭) vs Bag of Words (히스토그램 매칭)
	+ https://darkpgmr.tistory.com/125 
	+ Extract Features (e.g., SIFT) from Images + Learn Visual Dictionary (e.g., K-means clustering)
	+ Encode: Bags-of-Words (BOW) vectors for each image
	+ Classify: Train and test data using BOW vectors
	
+ Classifier
	+ K-Nearest Neighbor
	+ Naïve Bayes
	+ Support Vector Machine

#### Classification & Network
+ Convolution Networks 
+ Neural Networks
+ Convolutional Neural Networks 

#### Stereo Vision

+ 스테레오 영상을 처리하는데 있어 발생하는 이슈
	+ http://www.ntrexgo.com/archives/2280 

+ Two-view Geometry
	+ Epipolar Geometry
	+ Essential matrix
	+ Fundamental matrix

+ Stereo Vision
	+ Stereo Rectification
	+ Stereo Matching
	+ Disparity & Depth

#### Motion Tracking
+ Motion
	+ Image data is a function of space (x, y) and time (t)
+ Optical Flow   
	+ Lucas-Kanade
	+ https://overface.tistory.com/55
	+ Horn-Schunck
+ Visual Tracking
	+ Kanade-Lucas-Tomasi
	+ Mean shift tracking
	+ Cam shift tracking

#### Robot Vision
+ CV vs. MV vs. RV
+ Visual Servoing
+ Visual SLAM


### 자연어처리 


#### POS Tagging (with HMM)
+ Basic Probability Theory / Estimating Probability

+ POS Tagging 
	+ Language Model(1) - Bayesian Theorem 부터 Naive Bayes까지
	+ https://www.quantumdl.com/entry/Language-Model-Naive-Bayes?category=691797 
	+ Language Model(2) - HMM을 이용한 Part of Speech(POS) Tagging
	+ https://www.quantumdl.com/entry/Language-Model2-HMM%EC%9D%84-%EC%9D%B4%EC%9A%A9%ED%95%9C-Part-of-SpeechPOS-Tagging

+ HMM & Viterbi 
	+ [Algorithms for NLP] POS tagging, HMM, Viterbi
	+ https://m.blog.naver.com/ckdgus1433/221812095900 
	+ Baum–Welch algorithm
	+ https://hyunlee103.tistory.com/53?category=999732

#### 워드임베딩 (Word Embedding)
	
+ Basic idea of learning neural network word embeddings
	+ A loss (or cost) function : J = 1 − p(context|Wt) 
	+ Word2Vec (CBOW , SG)
	+ https://wooono.tistory.com/244
	+ Glove  
	+ https://ratsgo.github.io/from%20frequency%20to%20semantics/2017/04/09/glove/

+ Basic idea of learning neural network word embeddings 
	+ 딥 러닝을 이용한 자연어 처리 입문 - Pretrained-wordEmbedding
	+ https://wikidocs.net/33793

#### 딥러닝 기반의 자연어처리 모델 (RNN)

+ Convolutional Neural Networks

+ Recurrent Neural Networks (LSTM,GRU) - (static representation)
	+ Gated Recurrent Units
	+ https://dgkim5360.tistory.com/entry/understanding-long-short-term-memory-lstm-kr?category=912687
	+ Long Short-Term Memory (이해하기) 
	+ https://dhpark1212.tistory.com/entry/RNN-LSTM?category=817853

	
#### 딥러닝 기반의 자연어처리 모델 (Contextualized representation)

+ Seq2Seq
	+ https://blog.naver.com/sooftware/221784419691

+ Attention 메커니즘
	+ 의도 : 먼 거리에 위치한 단어의 가중치를 현재 단어에 효과적으로 반영할 수 있는 방법이 없을까? 
	+ https://blog.naver.com/sooftware/221784472231
	+ 각 입력이 출력 상태에 연결 신경망 추가 (모든 단어들의 가중치) / Attention 모델 - 구현편
	+ https://dhpark1212.tistory.com/entry/Attention-%EB%AA%A8%EB%8D%B8-%EA%B5%AC%ED%98%84%ED%8E%B8?category=817853
	
+ Transformer 구조
	+ 인코더가 입력 시퀀스를 하나의 벡터로 압축하는 과정에서 입력 시퀀스의 정보가 일부 손실된다는 단점
	+ 어텐션을 RNN의 보정을 위한 용도가 아니라 아예 어텐션으로 인코더와 디코더를 만들어보면 어떨까요?
	+ Transformer - Harder, Better, Faster, Stronger
		+ https://blog.pingpong.us/transformer-review/
	+ Transformer 설명
		+ https://dhpark1212.tistory.com/entry/Transformer?category=817853

+ Transformer 구조 - 더 나아간 모델들?
	+ BERT(Bidirectional Encoder Representations from Transformers)
		+ https://wikidocs.net/115055
	+ GPT(Generative Pre-trained Transformer)
		+ https://sooftware.io/gpt/
	+ BERT vs GPT
		+ https://ratsgo.github.io/nlpbook/docs/language_model/bert_gpt/
	+ Various Types of BERT (RoBerta, Albert) 
		+ RoBerta
		+ https://sooftware.io/roberta/	
		
+ (Feature-based approach) ELMo
	+ https://wikidocs.net/33930




### 머신러닝

+ Genetic Algorithm

+ NN
	+ from keras.datasets import mnist 
	+ from keras.datasets import imdb
	+ from keras.datasets import reuters
	+ from keras.datasets import boston_housing 


# 객체

+ 텐서플로우 객체 자료구조
	+ https://shinslab.tistory.com/107?category=898218

+ 파이토치 객체 자료구조
	+ https://statisticsplaybook.tistory.com/5

+ [ HelloWorld with CUDA ]
	+ https://mangkyu.tistory.com/84

# 파이썬

+ 내어쓰기  (Shift + TAB)


# 새로 찾기 

### 데이터셋 선정

+ GAN 
	+ 데이터셋 구축에서 GAN의 중요성  
	+ https://blog.testworks.co.kr/importance_of_gan_in_ai_dataset_building/ 

### 모델 선정


#### 모델 분류

+ Generative Model / Discriminative Model
	+ https://m.blog.naver.com/PostView.naver?isHttpsRedirect=true&blogId=ehdrndd&logNo=221520140545


#### 생성모델 

+ 생성모델이란 무엇일까? 
	+  https://minsuksung-ai.tistory.com/12

+ AutoEncoder (1) : Maximum likelihood 관점에서의 해석
	+ https://junstar92.tistory.com/156?category=905977
	+ 비지도 학습 문제를 지도 학습 문제로 바꾸어서 해결한 방법
	+ https://dhpark1212.tistory.com/entry/Autoencoder%EC%9D%98-%EB%AA%A8%EB%93%A0-%EA%B2%83?category=817853

#### 강화학습

+ 강화학습이란 무엇일까?
	+ https://minsuksung-ai.tistory.com/13 


### 최적화 

+ 최적화 기법의 직관적 이해
	+ https://darkpgmr.tistory.com/149
+ 함수최적화 기법 정리
	+ https://darkpgmr.tistory.com/142
 
### 추론

+ 딥러닝 모델 서비스 A-Z 1편 - 연산 최적화 및 모델 경량화
	+ https://blog.pingpong.us/ml-model-optimize/


### 챌린지

+ MATLAB 대학생 AI 챌린지 2021
	+ https://kr.mathworks.com/academia/student-challenge/2021/ai-challenge/winners.html?s_v1=41082&elqem=3570651_EM_KR_DIR_21-11_MOE-EDU&elqTrackId=4480e0b764164201a8c307d84d6af6a5&elq=ce9ed9c45b9d4cd79dcd36abc26bd796&elqaid=41082&elqat=1&elqCampaignId=15124



