
# 데이터사이언스 융합전공

https://sco.skku.edu/sco/community/major_data.do?mode=list&&articleLimit=10&article.offset=0

https://blog.pabii.co.kr/university-failure-in-data-science-education/

# 새로 익히기

+ 1. KNN / SMOTE
  + pass 
+ 2. linear regression matrix form
  + pass
+ 3. logistic regression 
  + pass
+ 4. SVM
  + 벡터연산 / 내적의 원리를 이용한 증명  
  + http://jaejunyoo.blogspot.com/2018/01/support-vector-machine-1.html 
  + https://angeloyeo.github.io/2020/09/30/SVM.html


+ 1. 나이브 베이지안
  + https://gomguard.tistory.com/69
+ 2. 히든 마코프 체인
  + https://ratsgo.github.io/machine%20learning/2017/03/18/HMMs/
+ 3. Viterbi 알고리즘
  + https://post.naver.com/viewer/postView.nhn?volumeNo=17867939&memberNo=11190852


+ 1. 엔트로피 & 크로스 엔트로피
  + https://hyunw.kim/blog/2017/10/14/Entropy.html
+ 2. KL Divergence
  + https://angeloyeo.github.io/2020/10/27/KL_divergence.html
  
  
+ 1. Hough Transform (line 찾기)
  + https://m.blog.naver.com/windowsub0406/220894462409 
+ 2. Harris corner detection (Corner 찾기) 
  + https://hello-stella.tistory.com/19
+ 3. Blob Detection (Gausian sigma 조절) 


# 파이선


# 알고리즘 


# 컴퓨터 비전

5주차 - 9/27(월)

+ local feature 
  + feature 찾는법? -> 표현법? -? 찾은 피쳐들 매칭법? 
  + feature 찾는법의 분류 : Edge , Line , Corner , Blob

+ detect/extract features
  + Edge detection
    + using derivatives (image gradient) - Difference Operator 
    + Prewitt operator, Sobel operator
  + Line detection
    + using edge detection & non-maximum suppression  
    + Canny operator 
    + Hough Transform (직선방정식 찾기) 
    + 엣지 디텍션에서 찾은 것들의 연결 
  + Corner detection
    + 점 하나는 기울기 하나/ 여러군데서 기울기 발생 (윈도우를 활용) 
    + Region Detection  
    + using a large change of intensity in any direction (significant change in all directions)
      + H matrix from the entries in the gradient (eigenvector)
    + Harris detector 
  + Blob detection
    + Finding characteristic region size (Laplacian of Gaussian looks bit like blob)
    + Laplacian of Gaussian
    

+ 1. Edge Detection 
  + 255 - 0  (기울기 -)
  + 0 - 255  (기울기 +)
  + 디지털화된 영상에서의 기울기? 
    + F[x+1,y] - F[x,y]  (이산적인 값의 차이를 빼준다)
    + 커널을 이용하여 계산  (X방향 ,Y 방향) 
    + X,Y 방향 모두의 기울기 = Gradient 
  + 나침반 8방향에 대한 Gradient 를 구하는 마스크 8개가 존재 ( 0 0 0 으로 기울기 방향 판단가능)
  
  + Edge detection : Filtering > Enhancement > Detection 단계를 하면서 Edge Detection 
    + Prewitt 
      + 마스크 -1 0 1 
      + Threshold 적용 
    + Sobel 
      + 마스크 -2 0 2 (가운데 가중치 2배)    
      + Threshold 적용
    + Robers cross operator
      + 대각선 성분 강하게 찾기 (2*2 마스크 : -1 1 교차) 
    
+ 2. Line Detection 
  + Canny 
    + 1. 이미지 가우시안 필터 이용 (노이즈 제거)
    + 2. Gradient 계산 (norm of gradient)
    + 3. Non maximum suppression (최대가 아닌걸 제거?)(NMS) 어떤 엣지를 골라야하나? 
    + 4. Two Threshold (low , high)   엣지들중에서 어떤식으로 스크리닝? 
  
  + 3. How to turn these thick regions of Gradient into Curves? (두꺼운 엣지 발생문제)
    + 여러개의 점들이 모인 엣지 = 어떻게 하나의 선으로? 
    + NMS 이용
    + 기울기가 여러(여러개점) 두꺼운 Edge 발생 (Max 점만 남기고 나머지는 suppression)
   
  + 4. NMS 이용하면 일부구간에서 Edge 가 끊기는 현상발생 
    + Hysteresis 방법이용 (low and high threshold)
    + High 이상은 당연히 쓰고 
    + low 와 high 의 중간을 보완하기 위한 방법 

+ Hough Transform
  + 찾아진 점들을 바탕으로 영상에서 라인을 찾는 방법 (직선,원,커브...)
  + ME) Active Contour 방법..?!
  + 점좌표 / 극좌표 방법
  
  + 점좌표 방법 : 
    + X,Y and A ,B (Y = AX+B) 모든 라인정보 투영? (a,b좌표 Space) 
    + 컴퓨터 : Array 생성 (A,B의 Max 값 설정)
    + 직선을 Array 에다가 A,B 좌표 Space Array에 1을 채우는 방식으로 진행  
    + 계속 Sum 해가며 가장 높은 값의 A,B 값 계산     
  
  + 극좌표 방법:
    + lo = Xcos(theta) + Ysin(theta) = 직선식 가능  
    + 0도를 중심으로 -90 ~ 89 기준 설정 (max min 자동산출)
    + lo 값은 가장 긴 대각선 길이? (영상의 크기로 산정가능)
    
+ 3. Corner Detection 
  + Region Detection 의 일종    
  + Corner : Significant Change in all directions
  
  + 찾는방법:
    + 1.각 점들의 Gradient 찾기
    + 2. Matrix 
    + 3. Eigen Vector , EigenValue
    + 4. 임계점 (Threshold 적용)
  
  + Harris Corner Detection 
  + W(x,y) : Window function
  + 윈도우가 움직였을때의 값의 차이 (I(x+u,y+u) - I (x,y))^2
  + 코너가 많이 움직인다 ? : 값의 차이가 크다 
    + 테일러 급수? : I(x+u,y+u) =  I(x,y) + Ix*u + Iy*y (x,y 방향 Gradient) 
    + 대입하여 정리? : (Ix*u+Iy*v)^2 = A*u^2 + 2B*uv + C*V^2 행렬연산으로 변환가능
    + Second moment matrix  : Ix^2 ,IxIy , Iy^2 으로 구성된 Matrix 
  
  + SVD (Singular value decomposition)
    + Eigenvector 주축 표현
    + Eigenvalue 주축의 크기 표현  (Horizontal , Vertical Edge 에 대한 )
    + 예시 (x y) (2 0  (x      = 2X^2+Y^2 =1 의 값 그래프 기울기 찌그러진 원 
                  0 1)  y)  
    + A*e = ramda *e 의 방정식을 풀면 det(A-ramda*I) = 0 
      + Flat , Edfe , Corner 의 Eigenvector , Eigenvalues 형태 주목하기!
      + 코너의 후보군을 이런 값을 기준으로 정할수 있다는 것 ! 
      + Use threshold on eigenvalues to detect corners. 

+ 4. Blob Detection 
  + Laplacian of Gaussian 
    + 2차 미분값 
  + 2차 미분값을 superposition 활용 중첩을 시켜서 미분값 maximum 값 활용
  + 가우시안 Sigma 값을 조절 <-> Peak 값 달라짐 <-> Blob 찾기 
  + 스케일을 찾는데 있어서의 한계점? : 필터를 키울수록 값이 작아짐..
    + Gausian filter 의 면적은 시그마에 반비례함 = 응답 시그널 크기 작아짐
  + 보정? : Normalization (시그마의 제곱을 곱해줌)
    + 보정한 결과 시그마에 영향을 받지않게 조절가능 
  + 각 스케일의 maximum mapping 을 3차원으로 나타내기 가능 
    + 예시 : 1시그마 = x,y 좌표 / 2시그마 ..3시그마 .... 각각 나타내기 가능
  + Laplacian 대신에 각 시그마의 이웃을 뺀 Difference of Gausian 값을 쓰기도 함 
    + 미분값(시그마에 대한 미분값) 대신 이산값(2시그마일때 x,y -1시그마일때 x,y) 사용   
    

# 자연어 처리 

5주차 - 9/27(월)

+ Stochastic Sequence Labeling
  + 1. 통계기법이 자연어 처리에서 활용되는 방법? Markov chain - 마르코프 체인
  + 2. 시퀀스 레이블 HMM   - 은닉 마르코프 모델 

+ Single classification vs sequential classifiacation 
  + 물고기 사진 검사 vs 단어의 순서적 배열의 적절성 평가? and 예측? 

+ Pattern recognition /  classification 모델 2가지 분류가능 (+ Generative vs Discriminate model )
  
  + https://sens.tistory.com/408 
  + P(Y|X) : X가 주어졌을때 Y (Class) 의 classification 
    + 1. P(X|Y) and P(Y) 이용하여 학습 -> P(Y|X) (Generative)  
      + ex) 나이브베이지안,HMM 
    + 2. P(Y|X) 를 바로 찾으려는 시도 (Discriminate model) - 확률 구하기가 힘드니까 Score 로 .. 
      + ex)MEM CRF(conditional random field) SVM DNN

+ 나이브 베이지안 

  + for Single classification 
  + P(y|x)  (x: variables , y : class variables)
  
  + P(x|y)*P(y) / P(x) (P(x): 상수취급, 전체 확률이 1이 아니여도 된다. ) 
  + P(y) : prior probability : 쉽게 계산가능
  
  + P(x|y) : 스팸메일 예시 : 스팸메일 feature vector 표시 / X = {광고,특판,이자,....}
    + P(Spam,X) and P(Non_spam,x) 계산하는 방법 : 
    + P(spam) and P(non_spam) 의 확률 : data에서 표본이 모집단의 확률로 가정하고 계산
    + P(X|Spam) and P(X|non_spam) 의 확률 : feature가 똑같은 패턴으로 나온 이메일 counting은 쉽지 않지
    + feature단위로 확률을 계산해야함 => multiply *(all) P(Xn|spam)
      + ex) P(광고|스팸) = 98/1000 (스팸메일 내부의 1000개 단어중에서 98개) 
      + 독립가정으로 단순 곱 진행
    
+ Markov chain
  + Sequence of random variables (that aren't independent)   
  
  + Markov Assumption
    + Xt-1 to Xt depends only on Xt-1 
    + 모든 확률 계산이 쉽지 않으니 바로 전 확률만 상태에 영향을 준다고 가정
      + Bigram model 예시 : P(X1) , P(X2|X1) , P(X3|X2) ........
        + Bigram model 로 언어를 예측하기가 쉽지않음
        + 한 단어만 주고 어떻게 예측을 하는가 (...)
      + Trigram model 예시 : P(X3|X2,X1) .....
  
  + 기본 모형
    + P(X1...Xt) = P(X1)P(X2|X1)P(X3|X1,X2)....P(Xt|X1...Xt-1)
  + Markov chain 모형 (기본모형에서 변형) 
    + P(X1)P(X2|X1)P(X3|X2).......P(Xt|Xt-1)
  + 나이브 베이지안 모형
    + P(X1)P(X2)...........P(Xt)  
  
  + Markov model 예시
    + 1. Transition state graph 에서의 중요성 : 갈래의 합은 1이 되어야한다 (주의)
    + 2. 날씨 예시 
  
+ Hidden Markov model 
  + 날씨 예제 : 전후 날씨를 볼수 있음 (visible)
  + why hidden?
    + you dont KNOW the state sequence that model passes through , 
    + 관찰하는 것과 state 가 같은 경우 : visible     
    + 형태소분석? : 형태소 분석에서의 상태는 품사 <-> 품사의 sequence에 의해 문장이 만들어짐  
      + I ate an apple (N+V+A+N)  (S+V+O)
      + 나는 단어를 보고 뒤에오는 sequence의 품사를 유추하는 것 
    
  + 이상한 음료수 자판기 예제
    + observation matrix. 
    + transition matrix.   
    
  + Notation for Hidden Markov models
    + T  = length of observation sequence (자판기 동작 횟수)
    + N = number of states in model (자판기 상태 수)
    + L = number of observation symnbols (자판기 음료 종류)
    + S = set of states (자판기 상태 집합)
    + A = state transition probability matrix (자판기 상태 변화)
    + B = Obsrvation probability distribution (음료수 확률 분포)
    + Pi = initial state distribution (초기 상태 분포)
    + Ramda = hidden markov model P(A,B,Pi)

6주차 - 10/04(월)

+ model(A,B,pi)
  + 자연어 처리 Sequence modeling : 
   
  + https://gritmind.blog/2020/08/30/viterbi_algorithm_pos/ 
  + 확률모델 3개의 table 이 필요 
  + Evaluation , (신호가 어디에서 왔는가?)
  + Decoding , (Word 에서 형태소 POS 분석하는것)  
  + Learning or estimation  (A,B Pi 를 어떤식으로 학습을 하는가?)

+ mu = (A,B,Pi) / O = (O1....Ot) 가 주어졌을때 compute P(O|mu) 계산하기 (주어진 조건 mu에서 O확률)  
  + X : Any state of sequence  
  + P(O,X) = P(O|X)*P(X) 적용 
  + P(O|mu) = P(O|X,mu)*P(X|mu) 변경 (B - (Observation state) * A - (State transition probability) 의미 동일)
  + state 수만큼 연산복잡도 증가 -> D.P 알고리즘 필요 (Viterbi 알고리즘)
  + https://sonsnotation.blogspot.com/2020/12/5-sequence-labeling.html
 
+ Viterbi 알고리즘 Forward / Backward
  + https://ichi.pro/ko/pumsa-pos-mich-viterbi-algolijeum-64172530316102
  + https://ratsgo.github.io/speechbook/docs/decoding/viterbi
 
+   
 
# 머신 러닝 모형

+ KNN (K-Nearest Neighbor)
  + SMOTE(synthetic minority oversampling technique) / (for data agumentation)
  + https://wordbe.tistory.com/entry/GAN-Data-Augmentation-Using-GANs-2019
  + https://www.kaggle.com/qianchao/smote-with-imbalance-data

5주차 - 9/27(월)
+ SVM (Support vector machine) + Kernel 

+ Constrained Optimizaion (제한된 조건에서의 최적화)
  + min f(x) =?  
  + when g(x) = 0 and h(x) <= 0     
+ Lagrange Function (라그랑주 승수법)
  + F(x,a,b) = f(x) + sum(a*g(x)) + sum(b*h(x))   
  + min x / max a,b
  + a: Lagrange multiplier(equal constraint) /  b : kkt multiplier(inequal constraint)
+ 만족하는 식
  + F(x,a,b) 미분값 = 0  
  + g(x) = 0
  + b*h(x) = 0
  + h(x) <= 0  

+ 예시 :  x^2+x^2 = 1 / subject x1+x2 = 1
  + 변형 : min max (x1^2+x2^2 + a(x1+x2-1)) 
  + 제한 조건 추가 :  있을때마다 a,b,c . . . . .
  + 만족하는 식 :
    +  x1^2+x2^2 + a(x1+x2-1) 미분값 (x1,x2) 편미분 : 식 2개
    +  x1+x2-1 = 0 
    +  3개의 식 획득 
  + 2x1+a = 0 / 2x2+a = 0 / x1+1x =1
  + 정답 : x1 = x2 = 0.5 , a =-1

예시 : x^2+x^2 = 1 / subject x1+x2 = 1 , x1>=2
  + 만족 식:
    + F (미분 편미분 식 2개)
    + x1+x2-1 = 0 
    + a(-x1+2) = 0 
    + -x1+2 <= 0 
  + 정답 : KKT mulitiplier 를 이용하여 a가 0일때 , 0이 아닐때로 나뉘어서 case 1 . 2 조합
  
+ 결과 정리 : kkt multiplier 수에 따라서 최악에는 2^p 의 subproblem 이 있을 수 있음 

+ Dual form SVM
+ min x and max a ,b = max a,b and min x 로 변경가능 

+ Support Vector Machine
  + 직선을 하나 그려서 Boundary 를 지정하고 싶다. 어떤식으로 선을 그어야 할까?
  + 마진을 최대화 하는 Boundary !
  
  + 마진을 계산하는법?
    + 직선식 wx+b=0 , wx+b > 0 , wx+b < 0 (w,b 찾기)
     + wx+b =1 , wx+b = 0 ,  wx+b = -1  
     + 두 직선사이의 거리 계산 (수직인 직선에서 직선들이 만나는 점들의 거리)
       + 직선위의 한점 a,b 일때 W(a-b) = 0    
       + OX+ and OX- vector 가정 (가장 단순하게 법선 벡터와 평행한)
       + 1. |X+ - X-| = M   
       + 2.  X+ = X- + a * w (w벡터방향) (:상수) 
       + 1과 2식을 조합하면
         + |X+ - X-| = M  = a|w|
            + wx+b = 1 식에다가 x+ = x- + aw 대입 
            + w(x- + a*w) + b = 1 
            + 전개하면 wx- + a*w*w+b = 1
            + 정리하면 a = 2/w*w (wx- + b = -1 대입)
            + w*w = |w| 이므로 (거리) 
            + m = 2/|w|
   + w가 0으로 갈때 margin 이 max
   + 포인트,label에 따라서 constraint 식 추가 가능
   
    + 예시 : wx+b >1 , wx+b<-1 
    + 예시 : D = (1,1,-1) ,(2,2,+1)
      + min 0.5(w1^2+22^2) 
      + subject to (w1+w2+b+1 <=0 and -2w1-2w2-b+1<=0)
      + F = 0.5(w1^2+22^2) + a(w1+w2+b+1) + a2(-2w1-2w2-b+1)
      + 만족하는 식 :
      + x1 , x2에 대해서 각각 미분한식을 만족 + 4개의 추가 조건 획득  
        + case1 a1 = 0 a2 = 0  
        + case2 ,case3 case4 ... 에 따라서 case 별로 대입해보기 

+ Dual foam 변형
  + pass
  
+ Non-linear SVM
  + 데이터를 고차원으로 mapping      
  + equality constraint , inequality constraint 
  + data X -> O(x)
  + inner product 

+ Kernel trick
+ 커널을 이용하여 mapping = inner product 줄이기? 
  + (x1*x2+1)^3 으로 간단하게 표헌가능 !
  + 간단하게 만드는 커널 Transform 형태 (연산수 줄이기)    

+ 커널은 뭐가 될수있어?
  + Mercer's condition 만족  

# 선형대수학 

+ Linear regression matrix form
  + 행렬 연산 추가 
  + 적용하기 
    + https://yganalyst.github.io/ml/ML_chap3-1/ 
  + 수학적증명
    + https://jangpiano-science.tistory.com/111 
  
+ SVM 
  + 벡터 연산 기초
    + https://koreanfoodie.me/425 
  + 적용하기
    + https://hleecaster.com/ml-svm-concept/
  + 수학적 증명
    + https://ratsgo.github.io/machine%20learning/2017/05/23/SVM/ 
  
+ Edge detection 
  +   (Gradient 변화 극대?)
  + harris detector 
  + 
    + https://sunshower76.github.io/cv(computervision)/2020/03/12/Feature-detector-2.-Harris-corner-detector/ 
  
# 통계학 

엔트로피 / 크로스 엔트로피
+ Cross entropy는 어떤 문제에 대해 특정 전략(확률분포)을 쓸 때 예상되는 질문개수에 대한 기댓값입니다. 
+ https://hyunw.kim/blog/2017/10/14/Entropy.html

KL divergence 콜벡 라이블러 발산 
+ KL-divergence는 p와 q의 cross entropy에서 p의 엔트로피를 뺀 값입니다. 결과적으로 두 분포의 차이를 나타냅니다.
+ https://angeloyeo.github.io/2020/10/27/KL_divergence.html

나이브 베이지안
+ 최적화 문제를 풀 때 분모의 P(B)P(B)는 결과에 영향을 미치지 않으므로 생략하거나 상수 K로 놓고 푼다 
+ https://gomguard.tistory.com/69

HMM 히든 마코프 모델
+ 한 상태(state)의 확률은 단지 그 이전 상태에만 의존한다는 것이 마코프 체인의 핵심
+ https://ratsgo.github.io/machine%20learning/2017/03/18/HMMs/


# 딥러닝 (이론&엔지니어링) 

# XAI (eXplainable AI)


# 과제 

+ https://shinminyong.tistory.com/34

- 불균형데이터의 예를 들어 설명하고 불균형데이터를 사용하여 판별할 때 생기는 문제 (10점)

+ 불균형데이터의 예시

- 불균형데이터 판별에서 발생하는 문제 해결을 위한 접근 방법들 (20점)
 
- SMOTE 방법의 설명 (알고리즘, 수식 등) (20점)
 
- SMOTE와 비교할 수 있는 최신의 방법에 대한 간략한 소개 (10점)
 
- 위 내용을 공부하며 팀구성원들이 SMOTE에 대하여 새롭게 알게 된 것 (10점)
