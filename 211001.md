
# 새로 익히기

+ 1. KNN / SMOTE
+ 2. linear regression matrix form
+ 3. logistic regression 
+ 4. SVM

+ 1. 나이브 베이지안
+ 2. 히든 마코프 체인

+ 1. 엔트로피
+ 2. 크로스 엔트로피
+ 3. KL Divergence


# 파이선



# 알고리즘 



# 컴퓨터 비전

5주차 - 9/27(월)

+ detect/extract features
  + Edge detection
    + using derivatives (image gradient)
    + Prewitt operator, Sobel operator
  + Line detection
    + using edge detection & non-maximum suppression 
    + Canny operator, Hough Transform
  + Corner detection
    + using a large change of intensity in any direction (significant change in all directions)
      + H matrix from the entries in the gradient (eigenvector)
    + Harris detector 
  + Blob detection
    + Finding characteristic region size (Laplacian of Gaussian looks bit like blob)
    + Laplacian of Gaussian
    
# 자연어 처리 

5주차 - 9/27(월)

+ Stochastic Sequence Labeling
  + 1. 통계기법이 자연어 처리에서 활용되는 방법? Markov chain - 마르코프 체인
  + 2. 시퀀스 레이블 HMM   - 은닉 마르코프 모델 

+ Single classification vs sequential classifiacation 
  + 물고기 사진 검사 vs 단어의 순서적 배열의 적절성 평가? and 예측? 

+ Pattern recognition /  classification 모델 2가지 분류가능 (+ Generative vs Discriminate model )
  
  + https://sens.tistory.com/408 
  + P(Y|X) : X가 주어졌을때 Y (Class) 의 classification 
    + 1. P(X|Y) and P(Y) 이용하여 학습 -> P(Y|X) (Generative)  
      + ex) 나이브베이지안,HMM 
    + 2. P(Y|X) 를 바로 찾으려는 시도 (Discriminate model) - 확률 구하기가 힘드니까 Score 로 .. 
      + ex)MEM CRF(conditional random field) SVM DNN

+ 나이브 베이지안 

  + for Single classification 
  + P(y|x)  (x: variables , y : class variables)
  
  + P(x|y)*P(y) / P(x) (P(x): 상수취급, 전체 확률이 1이 아니여도 된다. ) 
  + P(y) : prior probability : 쉽게 계산가능
  
  + P(x|y) : 스팸메일 예시 : 스팸메일 feature vector 표시 / X = {광고,특판,이자,....}
    + P(Spam,X) and P(Non_spam,x) 계산하는 방법 : 
    + P(spam) and P(non_spam) 의 확률 : data에서 표본이 모집단의 확률로 가정하고 계산
    + P(X|Spam) and P(X|non_spam) 의 확률 : feature가 똑같은 패턴으로 나온 이메일 counting은 쉽지 않지
    + feature단위로 확률을 계산해야함 => multiply *(all) P(Xn|spam)
      + ex) P(광고|스팸) = 98/1000 (스팸메일 내부의 1000개 단어중에서 98개) 
      + 독립가정으로 단순 곱 진행
    
+ Markov chain
  + Sequence of random variables (that aren't independent)   
  
  + Markov Assumption
    + Xt-1 to Xt depends only on Xt-1 
    + 모든 확률 계산이 쉽지 않으니 바로 전 확률만 상태에 영향을 준다고 가정
      + Bigram model 예시 : P(X1) , P(X2|X1) , P(X3|X2) ........
        + Bigram model 로 언어를 예측하기가 쉽지않음
        + 한 단어만 주고 어떻게 예측을 하는가 (...)
      + Trigram model 예시 : P(X3|X2,X1) .....
  
  + 기본 모형
    + P(X1...Xt) = P(X1)P(X2|X1)P(X3|X1,X2)....P(Xt|X1...Xt-1)
  + Markov chain 모형 (기본모형에서 변형) 
    + P(X1)P(X2|X1)P(X3|X2).......P(Xt|Xt-1)
  + 나이브 베이지안 모형
    + P(X1)P(X2)...........P(Xt)  
  
  + Markov model 예시
    + 1. Transition state graph 에서의 중요성 : 갈래의 합은 1이 되어야한다 (주의)
    + 2. 날씨 예시 
  
+ Hidden Markov model 
  + 날씨 예제 : 전후 날씨를 볼수 있음 (visible)
  + why hidden?
    + you dont KNOW the state sequence that model passes through , 
    + 관찰하는 것과 state 가 같은 경우 : visible     
    + 형태소분석? : 형태소 분석에서의 상태는 품사 <-> 품사의 sequence에 의해 문장이 만들어짐  
      + I ate an apple (N+V+A+N)  (S+V+O)
      + 나는 단어를 보고 뒤에오는 sequence의 품사를 유추하는 것 
    
  + 이상한 음료수 자판기 예제
    + observation matrix. 
    + transition matrix.   
    
  + Notation for Hidden Markov models
    + T  = length of observation sequence (자판기 동작 횟수)
    + N = number of states in model (자판기 상태 수)
    + L = number of observation symnbols (자판기 음료 종류)
    + S = set of states (자판기 상태 집합)
    + A = state transition probability matrix (자판기 상태 변화)
    + B = Obsrvation probability distribution (음료수 확률 분포)
    + Pi = initial state distribution (초기 상태 분포)
    + Ramda = hidden markov model P(A,B,Pi)

  
# 머신 러닝 모형

+ KNN (K-Nearest Neighbor)
  + SMOTE(synthetic minority oversampling technique) / (for data agumentation)
  + https://wordbe.tistory.com/entry/GAN-Data-Augmentation-Using-GANs-2019
  + https://www.kaggle.com/qianchao/smote-with-imbalance-data

5주차 - 9/27(월)
+ SVM (Support vector machine) + Kernel 


# 선형대수학 

+ Linear regression matrix form
  + 행렬 연산 추가 
  + 적용하기 
    + https://yganalyst.github.io/ml/ML_chap3-1/ 
  + 수학적증명
    + https://jangpiano-science.tistory.com/111 
  
+ SVM 
  + 적용하기
    + https://hleecaster.com/ml-svm-concept/
  + 수학적 증명
    + https://ratsgo.github.io/machine%20learning/2017/05/23/SVM/ 
  
+ Edge detection 
  +   (Gradient 변화 극대?)
  + harris detector 
    + https://sunshower76.github.io/cv(computervision)/2020/03/12/Feature-detector-2.-Harris-corner-detector/ 
  
# 통계학 

엔트로피 / 크로스 엔트로피
+ Cross entropy는 어떤 문제에 대해 특정 전략(확률분포)을 쓸 때 예상되는 질문개수에 대한 기댓값입니다. 
+ https://hyunw.kim/blog/2017/10/14/Entropy.html

KL divergence 콜벡 라이블러 발산 
+ KL-divergence는 p와 q의 cross entropy에서 p의 엔트로피를 뺀 값입니다. 결과적으로 두 분포의 차이를 나타냅니다.
+ https://angeloyeo.github.io/2020/10/27/KL_divergence.html

나이브 베이지안
+ 최적화 문제를 풀 때 분모의 P(B)P(B)는 결과에 영향을 미치지 않으므로 생략하거나 상수 K로 놓고 푼다 
+ https://gomguard.tistory.com/69

HMM 히든 마코프 모델
+ 한 상태(state)의 확률은 단지 그 이전 상태에만 의존한다는 것이 마코프 체인의 핵심
+ https://ratsgo.github.io/machine%20learning/2017/03/18/HMMs/


# 딥러닝 (이론&엔지니어링) 

# XAI (eXplainable AI)

