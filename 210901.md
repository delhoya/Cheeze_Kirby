
# 파이선

+ 파이선 중첩 클래스 (클래스 내부 클래스) 
  + https://www.delftstack.com/ko/howto/python/nested-class-in-python/
 
+ 파이선 F-string 소수점 표기법

+ #Input
number = 2.341
+ #세미콜론 사용 주의 (변수명 : .2f)
print(f'{number:.2f}')
+ #Output
2.34


# 자료구조 


# 알고리즘

+ https://hyunw.kim/blog/2018/09/18/Algorithm_Analysis01_Intro.html


# 딥러닝(케라스)

+ 케라스 강의
  + https://tykimos.github.io/lecture/

+ 케라스의 구현 방식: Sequential API / Functional API / Subclassing API
  + https://wikidocs.net/106897 
  
+ 핸즈온 머신러닝
  + https://formal.hknu.ac.kr/handson-ml2/ 
   

# 딥러닝(파이토치)

+ computational graph(연산그래프) 동적 vs 정적
  + https://bookandmed.tistory.com/57
  + 1. 텐서플로우 : static graph로 처음에 구성한 computational graph를 매 iteration마다 재사용한다.
  + 2.  파이토치 : dynamic graph로 매 iteration 수행시마다 새롭게 computational graph를 생성한다.


# 수학

+ 자연상수 e
  + https://smartbooks1.tistory.com/m/1625 
 
+ 벡터 
  + 벡터의 내적 : 벡터의 유사도 측정 (방향이 같으면 + , 다르면 - )  
    + 기하학적 정의 (cosine) 
  + 두 벡터의 단위벡터 내적 
    + 코사인 값(코사인유사도) 획득 가능 

+ 행렬
  + 전치행렬 Transpose matrix 
    + At = A    : 대칭행렬
    + At = A^-1 : 직교행렬
  
+ 선형변환
  + 행렬변환 <-> 선형변환   (U+V , kU)
    + matrix <-> linear 
  
+ 함수와 미분
  + 함수 , 합성함수 , 손실함수 (loss function)
    + 미분    
    + 손실함수  / SGD 경사하강법 
    + Chain Rule  
    
+ 확률

  + 과거의 일어난 일(Experiment , Trial) -> 미래의 일어날 일을 prediction 하고 싶은 것 (확률은 도박사로부터 시작!) 
  + 관심있는 사건을 Counting 하기 위해 경우의 수 (Permutation ,combination) 을 배움 
  
  + Sample Space : 나올수 있는 Output의 전체 집합
  + Event : Sample space 의 subset (부분집합) ex coin toss head , tail ..
  
  + Probability :  
    + Estimating probability :
      + 전체에서 데이터를 얻을수는 없음 (Unknown constant) : 추정이 필요 
      + 실험을 여러번 나뉘어서 진행 (8000번 -> 1000 * 8)
    + Conditional Probability
      + 조건부확률
      + P(AnB) = P(BnA) 에서 출발 
      + P(B) * P(A|B) = P(A) * P(B|A)    
      + 조건부 확률을 바꿀수 있음 (역확률의 계산가능) 
    + Independence 
      + A와 B 사건이 독립이라면
      + P{B|A) = P(B)
      + P(AnB) = P(A) * P(B) 
    + Chain Rule 
      + https://www.seas.upenn.edu/~cis391/Lectures/probability-bayes-2015.pdf 
      + P(A,B,C,D,E) = P(A|B,C,D,E) P(B,C,D,E) 
      +              = P(A|B,C,D,E) P(B|C,D,E) P(C,D,E)
      +              . . . .
                     = P(A|B,C,D,E) P(B|C,D,E) P(C|D,E) P(D|E) P(E)
    + Golden Rule
      + P(A,B) = P(B|A) P(A) 
      + P(B|A) = P(A|B) * P(B) / P(A) 
      + Posterial Probability : 사후확률  P(B|A)
      + Prior Probability : 사전확률 P(B)
      + Likelyhood  : 우도 P(A|B)
      + Evidence : 증거 P(A)
      + 사용예시 : A : Object , B : Class
    + Random variable
      + Function : Sample space -> Real number 
      + P(x) : X (Event : Subset) 
    + Expectation(평균) and Variance(분산)
      + Expectation 평균  
      + 데이터의 분포 상태를 평균값만으로 표현이 가능한가 ? (어떤 형태로 모여서 데이터가 이뤄져있는가?)
      + 양쪽 데이터에서 기대값의 차이(거리)를 계산한 후 제곱하여 계산 
      + Variance 분산
        + V(x) = E(X^2) - E^2(x)      
  
  + 자연어 처리 & 확률
  + P is unknown , Sample of data (Corpus : 글이 모여있는 데이터)
    
   + Frequentist Stastics :
    + 사건이 일어날 경우의수 /전체
    + Fu = C(u) / N 
    + C(u) : Number of time n occur in N trials 
      + N이 무한대로 갈 경우 확률이 수렴 
      + ex 한국어에서 명사가 나올 확률 : 아무리 데이터를 다 모아도 전체 N 이라고 할수있을까? Nope.
      + 내가 모은 것만 가지고 Frequentist 적인 방법으로 추정하기 (데이터가 많을수록 ideal probability)
   
   + probability :    
        + Parametric
          + 연구하려는 분야의 데이터 분포를 이미 알고있다면 (Well known)  
          + 기존의 연구된 특성을 그대로 이용가능 (ex Binomial , normal distribution) 
          + Binomial Distribution (2개의 output)를 가지고 trial (각각 독립적)
            + ex coin toss : n번 중에 10번이 앞면일 확률? 

        + Non-Parametric
          + 데이터 분포의 특징을 모르는것 
          + 하나씩 Count 하는수 밖에 없는것 (Distribution free) 
        
    + MLE (Maximum likelyhood estimation) 
    + 데이터를 가지고 확률 모델을 뽑았을때? : 확률 모델(Theta)이 데이터세트(D)를 가장 잘 설명할수있으면 좋겠다 (Maximum) 
      + theta* = Argmax P(D|Theta) 
      
        
+ 정보이론
  + https://hyunw.kim/blog/2017/10/14/Entropy.html 

  + A -> B : 정보를 전송할때 과연 몇 bit (정보량)이 필요할까?
  + Entropy : Measure of uncertainty (불확실성)  
  + ex A(4bit) -> B(????) : B는 A에 대해서 잘 모름 (알아야 하는/전달 받아야하는 정보량이 많음) 
  + 질문 : 스무고개 (4*4 card)
    + 반반으로 나뉠수 있는 질문이 가장 효율적 : 이상적인 Case
    + 4번의 질문? : 1. 검정색/빨간색 카드인가 ? (색에 대한것)  2. 숫자가 짝수냐 홀수냐? 3.스페이스냐? Yes/No 4. 2냐? yes/No 
  
  + Fomula : 
    + W = 2^n (n: 정보량) (지수가 2인 이유: bit)  
    + logW = nlog2  , n = logW/log2
    + w대신에 P(확률)을 적용 : P = 1/w = 1/2^n 
    + n = -logP / log2  = -log2P (밑이 2인 로그) 
  + log 함수 응용
    + 확률 P1 P2... Pn 을 계속 곱하면 0으로 수렴 (컴퓨터 : underflow)
    + log 함수로 계산하면 logP1 +logP2 + ... 덧셈식으로 계산가능
  
  + Fomula2 (엔트로피 식의 확장) 
  + ex 8개의 빨간 공 / 2개의 흰 공
    + 전체 엔트로피 : -log2P 덧셈식 활용 
    + 확률의 평균값 + log2P 덧셈 
  + Entropy 
    + Entropy = P(x)log2 (1/P(x))
    + P(x) : 확률
    + log2(1/P(x)) : 정보량
    + 엔트로피는 항상 0보다 크다 
    + Variable 이 2개 이상일경우
      + entropy = -P1 log P1 - P2 log P2
  + Joint Entropy 
    + pass 
    + H(x,y) = sum of P(x,y)log(x,y) (x,y의 합계)
  + Conditional Entropy 
    + pass
    + H(y|x) = sum of P(x)H(y)  = P(x,y)log(y|x)
  + Entropy Chain Rule
    + pass
  + Mutual information 
    + pass 
    + 서로 얼마나 정보를 공유하는가? 
  + Language model and entropy
    + P(x) : 
    + q(x) :
    + 둘이 얼마나 잘 맞는지 엔트로피 공식 활용 KL divergence 
    + 분포의 distance가 얼마나 다른지? (KL divergence) 
      + https://hwiyong.tistory.com/408    
  
# 자연어 처리 

+ 1주차 강의 

  + 자연어 처리 파이프라인  
    + 1. Preprocessing (Sentence extraction(한 문장추출), word spacing(띄워쓰기), normalization(정규화->하나의 Form ex 3 -> 삼 )) 
      + 시스템 전처리 , Input 오류 보정 ,  
      + 문서 -> 단락 -> 문장 -> 어절 (띄워쓰기 단위)-> 음절 (초성+중성+종성) (을) -> 음소 (ㅇ+ㅡ+ㄹ)  
    + 2. Morphology 
      + 형태소 분석 (Part of speech) (POS tagging)
      + 형태소 : 어절과 음절의 사이 ? : 한 형태소는 여러개의 음절로 이뤄짐 
      + 단어를 계속 쪼갰을때 의미(Meaning)를 가지는 단위 : 
      + ex) 엔진을 -> 엔진(명사) + 을(조사)
    + 3. Syntax 
      + 문법요소 분석  
    + 4. Semantic 
      + 의미 관계 분석 
      + 동사와의 관계를 통해서 분석 가능   
      + 단어의 의미 분석 ex 사과를 먹을순 있지만 연필은 먹을수는 없지 
    + 5. Discourse
      +  담화분석
      +  1문장의 의미 + 앞에 나온 문장에 따라서 의미가 달라질 경우
      +  밥 먹었니? 네 
      +  밥 안먹었니? 네
      +  네의 의미가 앞문장에 따라 계속 달라짐. 
    + 6. NER - natural language processing  
      + 고유명사를 어떤식으로 추출할까요 ?
      + PLO (사망 지역 조직명) 
      + 스티브잡스(사람)이 설립한 애플(회사)와 가까운 맥도날드 캘리포니아(지역)지점 에서 애플(사과)파이 먹기
        
     
+ 2주차 강의 (9/6)
  + 자연어처리 파이프라인
    + 형태소 분석, 구문 분석, 의미 분석, 담화분석 (NLU)
    + 문장생성  (NLG)
  + 모호성 존재 
    + 품사 / 단어의 의미가 변동성 가짐 (문맥 , common sense의 중요성 가짐) 
  + 발전의 단계?
    + Rule based Model  -> 분기문 Model  -> 확률 Model (베이지안 Classifier) / Hidden Markov /  -> Deep learning     
  + NLP의 2가지 단계 
    + Natural Ranguage Understanding(NLU) + Generation(NLG)

  + 자연어처리 머신러닝 기법?
    + Classification : N21 (MLP SVM model ,CNN Model)
    + Sequential Labeling : N2N (ex) POS tagging , CRF Model , RNN Model ) 
    + Seq to Seq : N2M (Generation model) 
    + N2 structure (의존관계, 트리?) :  


# 기계 학습

+ 1주차 강의 
  + K Nearest neighbors  
    + classification
      + 데이터가 주어졌을때 어떤 class에 속해있나? 
      + 가장 가까운 친구 찾기 나는 누구인가 - 유유상종 알고리즘 
      + K 에 따라 label이 변할수 있음   
    
    + Regression
      + 출력이 class가 아니라 실수값으로 나옴
      + X 입력축을 기준으로 K개 가까운 X 찾고 , 그 X들의 Y 값의 평균으로 출력     
    
    + variation
      + 단순하게 하지말고 거리까지 고려할 필요가 있지 않을까?  (Not just counting)
      + 거리에 따른 가중치 W(x) 적용 , 함수 디자인 
        + 1.  W(x) 함수 디자인 , 거리반비례 : 1/distance(x1,x2)
        + 2.  Exponential 함수 이용   
        + 3.  커널 리그레션 : 가우시안 분포 적용 (?!)
        + https://data-science-hi.tistory.com/64
         
    + Distance Measure
      + kNN 의 거리를 어떤식으로 정의? 
        + 공간정의에 따라 달라짐
        + 유클리안 거리 : 공간의 span에 따라 거리가 달라질수있음
        + 다른 거리 측정법? : 코사인 유사도 , 피어슨 correlation .., etc 
        + https://dive-into-ds.tistory.com/46
         

+ 2주차 강의  
  + Linear regression
    + W vector (w0,w1,w2....wn) 찾기   
    + X vector (X1,X2,......Xn)
    + loss function : MSE(Mean Squared Error)
  
  + how to ? : 모든 W에 대해서 편미분 진행 (variable수와 같은수의 equation 생성)   
  + how to solve ? : 식을 전개한것을 matrix 에 적용 w = A-1*B 적용
  + w = (XtX) -1 * (XtY)  (X transpose() )
  
    + linear regression matrix form 
      + https://hyeshinoh.github.io/2018/11/21/math-lin_alg-017/
      + https://jangpiano-science.tistory.com/111
    
    + 단순선형회귀분석 / 정규방정식
      +  https://mazdah.tistory.com/831 
      +  https://omicro03.medium.com/linear-regression-63164cd2e51     
    
    + kernel regression matrix  
      + https://analysisbugs.tistory.com/163   
  
  + Another solving : Gradient descendant , Maximum likelyhood 
  
  + Model analysis: 
    + 1. Overfitting vs Generalization   
    + 2. 성능평가 : Training set and Test set 
    + 3. Validation : K fold Validation + ETC  
      + https://deep-learning-study.tistory.com/623

+ 3주차 강의 
  + 

# 컴퓨터 비전

+ Computer Vision: Algorithms and Applications, 1st ed.
  + https://szeliski.org/Book/1stEdition.htm
  + http://www.cse.psu.edu/~rtc12/CSE486/

+ 1주차 강의 : 강의소개 
  
+ 2주차 강의(9/7) : Geometric camera model  (Plane to Plane Mappings)
  
  + 좌표계
    + 동형좌표계  
      + https://darkpgmr.tistory.com/77 
   
    + Image transformation
      + 이미지 변환시 자유도 계산 (DOF) 
      + https://elecs.tistory.com/151
  
  + Geometric camera model 
    
    + 외부세계 (3D) - > 카메라 -> 사진 (2D) : 
      + 사영기하학 
      + 소실포인트 : 큐브의 각도 소실 / 길이 / 평행선 소실 ex vanishinh point (철로사진 예시 )
      + 유지포인트 : 큐브의 선들의 교차점 유지  
    
    + Pinhole camera 
      + 빛에 의한 객체의 모든 정보량을 필름에 감광 X : 한 포인트만 뚫려있는 배리어(조리개)를 앞에 둔다 
      + 핀홀이크다 : 정보량 겹침(blur) / 핀홀이 너무 작다 : 정보량 너무 작음 (blur) + 회절의 문제점 발생
      
      + Pinhole Camera Matrix model : 3*4 matrix
        + https://staff.fnwi.uva.nl/r.vandenboomgaard/IPCV20162017/LectureNotes/CV/PinholeCamera/PinholeCamera.html 
        + 1. 렌즈의 공식 : (x,y,z) -> (f * x/z , f * y/z) 
        + 2. 중심점 보정 : (이미지 and detector) : Ox, Oy 값 추가로 넣어줌
        + 3. Rotation , Translation 보정
        + 4. 추가 자유도 : 카메라가 정사각형이 아닐수도 있음 (자유도 2 추가) + Skew 된 카메라일경우? (자유도 1추가) 
                 
    + Lens camera
      + 반사되는 여러가지 빛들을 효율적으로 모아서 전달 
      + 초점 : Focal point (빛이 모이는 지점) (빛의 One point (볼록렌즈 생각)) 
      + 장점 : 빛이 잘 모여질때 (초점) 잘 맞을때  (focal point: 초점) 
      + 단점 : 제대로 초점이 안 맞을때 
      + 렌즈의 모양을 바꿔가면서 진행 (눈과 같은 원리)  
      + 렌즈의 공식 : 
        + Object 크기 : y
        + Object 크기 (렌즈상) : y'  
        + distance (D): 사물 - 렌즈거리 
        + focal point (f) : 렌즈 - 초점 거리 
        + focal distance (D') : 초점거리 - 필름(Detector) 거리 
        + 1/D + 1/D' = 1/f (기하학을 이용하여 유도가능)  
      
     + Depth of Field : 피사계 심도 
        + D 가 커질수록 : D' 과 f 는 같아짐 
        + 어떤식으로 컨트롤이 가능 ? : 조리개 사이즈로 컨트롤 가능. 
        + 피사계 심도가 얇다 = 배경과 피사체 분리가능 (배경 흐릿) 
        + https://mgun.tistory.com/1388
        
      + Field of View 
        + Fov = tan-1(d/2f) 
        + f가 길어질수록 시야각은 좁아진다 
      
      + Focal length vs Viewpoint       
        + 멀면 멀수록 배경을 crop 시키는 효과 ? 
        + https://www.sony.ca/en/electronics/focal-length-angle-of-view-perspective
      

+ 3주차 강의 (9/13)
+ What is image? 
  + how to reduce noise? / how to find useful feature ?
  + Image warping : 화소의 위치 변경
  + Image filtering : 화소의 값 변경 
    + point operation  : 변화를 주는 함수 / 픽셀 자체에만 적용 
    + Filter operation : 하나의 화소뿐만 아니라 주변에 비해 얼만큼 조절할것인가? : Neighborhood of each pixel    
  + Linear shift Invariant Image filtering
    + 하나의 이미지에 Kernel & Stride 동일하게 적용  
    + 어떤 kernel 을 만들어야하는가? 
      + Square mean filter : 가중치의 합의 평균으로 나누어줌 (blur 처리 효과)
        + Same padding  : (for 같은영상크기 : Ps = (kernels-1) /2  : 커널 사이즈3일경우 : 1패딩  
        + Valid padding : Fs(6*6 이미지) -Ps(3*3 커널사이즈) + 1 = 6-3+1 = 4  
        + 필터 조합해도 선형변환이므로 따로 연산 가능
        + shift Invariant : https://en.wikipedia.org/wiki/Shift-invariant_system
      + Mask 사이즈의 크기는 어떤식으로 정하나?
        + 마스크의 크기가 클수록 더 Blur : 주변을 더 고려 
        + more expensive to compute : 연산량 up
        + bigger noise spread        
      + Mask 원형으로 구성 가능 : (to eliminate edge effect) 
  
  + Convolution Filtering
    + pass
    + pass


# 의료 영상 

+ https://opencv.org/kornia-an-open-source-differentiable-computer-vision-library-for-pytorch/

+ Affine Matrix
  + http://www.sci.utah.edu/~acoste/uou/Image/project3/ArthurCOSTE_Project3.pdf
  + https://en.wikipedia.org/wiki/Affine_transformation
  + https://velog.io/@nayeon_p00/OpenCV-%EC%96%B4%ED%8C%8C%EC%9D%B8-%EB%B3%80%ED%99%98
  + https://gaussian37.github.io/vision-concept-geometric_transformation/
  + https://m.blog.naver.com/PostView.naver?isHttpsRedirect=true&blogId=samsjang&logNo=220504966397    
  + https://opencv-python.readthedocs.io/en/latest/doc/10.imageTransformation/imageTransformation.html

+ Image Agumentation
  + https://analysisbugs.tistory.com/266?category=972884  




# 북마크

+ https://www.philgineer.com/2020/10/awesome-machine-learning.html
+ https://wonjun.oopy.io/70a8f11f-5728-45e8-a1a5-e6303bf56767
+ https://bbongcol.github.io/deep-learning-bookmarks/
+ https://ddmix.blogspot.com/2015/11/favorites-bigdata-ml.html

# 책

+ https://d2l.ai/index.html 

+ https://shb.skku.edu/bigs/menu3/sub01.jsp#l1
