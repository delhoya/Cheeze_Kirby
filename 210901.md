
# 파이선

+ 파이선 중첩 클래스 (클래스 내부 클래스) 
  + https://www.delftstack.com/ko/howto/python/nested-class-in-python/
 
+ 파이선 F-string 소수점 표기법
  + https://www.codeit.kr/community/threads/10499

+ 파이선 리스트 컴프리헨션
  + https://wikidocs.net/84393 


# 자료구조 

+ https://kingpodo.tistory.com/category/%EC%9E%90%EB%A3%8C%EA%B5%AC%EC%A1%B0?page=3

  + 스택 / 큐 / 덱 
  + 이진탐색 트리
  + 힙 트리
    + https://gmlwjd9405.github.io/2018/05/10/algorithm-heap-sort.html
  + AVL 트리
    + https://m.blog.naver.com/PostView.naver?isHttpsRedirect=true&blogId=dhdh6190&logNo=221062784111
  + 그래프 


# 알고리즘

+ 알고리즘 책 
+ http://jeffe.cs.illinois.edu/teaching/algorithms/

+ 알고리즘 강의 
+ https://www.youtube.com/playlist?list=PLtqbFd2VIQv4O6D6l9HcD732hdrnYb6CY
+ https://blog.encrypted.gg/category/%EA%B0%95%EC%A2%8C/%EC%8B%A4%EC%A0%84%20%EC%95%8C%EA%B3%A0%EB%A6%AC%EC%A6%98?page=2


+ 시간 / 공간 복잡도
  + N의 크기에 따라 시간복잡도가 달라짐
  + 허용크기 N
    + N <   10  : O(N!)
    + N <   25  : O(2^N)
    + N <  100 :  O(N^4)
    + N <  500 :  O(N^3)
    + N < 3000 :  O(N^2logN)
    + N < 5000 :  O(N^2)
    + N < 1,000,000 : O(NlogN)  : 힙정렬 (2nlogn+n) 
    + N < 10,000,000 : O(N)
    + N 그 이상 : O(logN) ,O(1)  

+ 그리디 알고리즘
  + 크루스칼
  + 다익스트라 
  + 배수로 떨어지는 동전 

+ 재귀함수 
  + n , n-1 만 다른 자신을 반복 호출 
  + 반복해서 작아지며 0(초기값 = 종료시점) 으로 수렴
  + 하노이의 탑 (막대기 3개 필요 , A1=1  ((An+1) = 2(An) + 1 )
  
+ 백트래킹
  +  트리로 선택지 고르기 
  +  vs 깊이 우선 탐색 :후보군을 미리 제외하여 Stack에 넣어주는 알고리즘 추가 
  
+ 분할정복
  + 작은 문제 나누기 divide and conquer
  + https://sinseonc.tistory.com/10

+ 다이나믹 프로그래밍 점화식
  + 문제 나누기 but 반복 연산 일때   
  + 반복 호출을 줄이기 위해 메모해가면서 연산량 줄이기 


# 딥러닝(케라스)

+ 케라스 강의
  + https://tykimos.github.io/lecture/

+ 케라스의 구현 방식: Sequential API / Functional API / Subclassing API
  + https://wikidocs.net/106897 
  
+ 핸즈온 머신러닝
  + https://formal.hknu.ac.kr/handson-ml2/ 
   

# 딥러닝(파이토치)

+ 텐서플로우 vs 파이토치 
+ computational graph(연산그래프) 동적 vs 정적
  + https://bookandmed.tistory.com/57
  + 1. 텐서플로우 : static graph로 처음에 구성한 computational graph를 매 iteration마다 재사용한다.
  + 2.  파이토치 : dynamic graph로 매 iteration 수행시마다 새롭게 computational graph를 생성한다.

+ 대용량 데이터 코사인 유사도
  + FAISS 알고리즘  
  + https://velog.io/@jinho0705/Faiss-Product-Quantization

# 수학

+ 자연상수 e
  + https://smartbooks1.tistory.com/m/1625 
 
+ 벡터 
  + 벡터의 내적 : 벡터의 유사도 측정 (방향이 같으면 + , 다르면 - )  
    + 기하학적 정의 (cosine) 
  + 두 벡터의 단위벡터 내적 
    + 코사인 값(코사인유사도) 획득 가능 

+ 행렬
  + 전치행렬 Transpose matrix 
    + At = A    : 대칭행렬  (A*At = A*A)
    + At = A^-1 : 직교행렬  (A*At = E  ) 
  
+ 선형변환
  + 행렬변환 <-> 선형변환   (U+V , kU)
    + matrix <-> linear 
  
+ 함수와 미분
  + 함수 , 합성함수 , 손실함수 (loss function)
    + 미분    
    + 손실함수  / SGD 경사하강법 
    + Chain Rule  
    
+ 확률

  + 과거의 일어난 일(Experiment , Trial) -> 미래의 일어날 일을 prediction 하고 싶은 것 (확률은 도박사로부터 시작!) 
  + 관심있는 사건을 Counting 하기 위해 경우의 수 (Permutation ,combination) 을 배움 
  
  + Sample Space : 나올수 있는 Output의 전체 집합
  + Event : Sample space 의 subset (부분집합) ex coin toss head , tail ..
  
  + Probability :  
  
    + Estimating probability :
      + 전체에서 데이터를 얻을수는 없음 (Unknown constant) : 추정이 필요 
      + 실험을 여러번 나뉘어서 진행 (8000번 -> 1000 * 8)
      
    + Conditional Probability
      + 조건부확률
      + P(AnB) = P(BnA) 에서 출발 
      + P(B) * P(A|B) = P(A) * P(B|A)    
      + 조건부 확률을 바꿀수 있음 (역확률의 계산가능) 
      
    + Independence 
      + A와 B 사건이 독립이라면
      + P{B|A) = P(B)
      + P(AnB) = P(A) * P(B) 
      
    + Chain Rule 
      + https://www.seas.upenn.edu/~cis391/Lectures/probability-bayes-2015.pdf 
      + P(A,B,C,D,E) = P(A|B,C,D,E) P(B,C,D,E) 
      +              = P(A|B,C,D,E) P(B|C,D,E) P(C,D,E)
      +              . . . .
                     = P(A|B,C,D,E) P(B|C,D,E) P(C|D,E) P(D|E) P(E)
                    
    + Golden Rule
      + P(A,B) = P(B|A) P(A) 
      + P(B|A) = P(A|B) * P(B) / P(A) 
      + Posterial Probability : 사후확률  P(B|A)
      + Prior Probability : 사전확률 P(B)
      + Likelyhood  : 우도 P(A|B)
      + Evidence : 증거 P(A)
      + 사용예시 : A : Object , B : Class
      
    + Random variable
      + Function : Sample space -> Real number 
      + P(x) : X (Event : Subset) 
      
    + Expectation(평균) and Variance(분산)
      + Expectation 평균  
      + 데이터의 분포 상태를 평균값만으로 표현이 가능한가 ? (어떤 형태로 모여서 데이터가 이뤄져있는가?)
      + 양쪽 데이터에서 기대값의 차이(거리)를 계산한 후 제곱하여 계산 
      + Variance 분산
        + V(x) = E(X^2) - E^2(x)      
  
  + 자연어 처리 & 확률
  
  + P is unknown , Sample of data (Corpus : 글이 모여있는 데이터)
    
   + Frequentist Stastics :
   
    + 사건이 일어날 경우의수 /전체
    + Fu = C(u) / N 
    + C(u) : Number of time n occur in N trials 
      + N이 무한대로 갈 경우 확률이 수렴 
      + ex 한국어에서 명사가 나올 확률 : 아무리 데이터를 다 모아도 전체 N 이라고 할수있을까? Nope.
      + 내가 모은 것만 가지고 Frequentist 적인 방법으로 추정하기 (데이터가 많을수록 ideal probability)
   
   + probability :    
   
        + Parametric
          + 연구하려는 분야의 데이터 분포를 이미 알고있다면 (Well known)  
          + 기존의 연구된 특성을 그대로 이용가능 (ex Binomial , normal distribution) 
          + Binomial Distribution (2개의 output)를 가지고 trial (각각 독립적)
            + ex coin toss : n번 중에 10번이 앞면일 확률? 

        + Non-Parametric
          + 데이터 분포의 특징을 모르는것 
          + 하나씩 Count 하는수 밖에 없는것 (Distribution free) 
        
    + MLE (Maximum likelyhood estimation) 
      + 데이터를 가지고 확률 모델을 뽑았을때? : 확률 모델(Theta)이 데이터세트(D)를 가장 잘 설명할수있으면 좋겠다 (Maximum) 
        + theta* = Argmax P(D|Theta) 
      
        
+ 정보이론

  + https://hyunw.kim/blog/2017/10/14/Entropy.html 

  + A -> B : 정보를 전송할때 과연 몇 bit (정보량)이 필요할까?
  + Entropy : Measure of uncertainty (불확실성)  
  + ex A(4bit) -> B(????) : B는 A에 대해서 잘 모름 (알아야 하는/전달 받아야하는 정보량이 많음) 
  
  + 질문 : 스무고개 (4*4 card)
    + 반반으로 나뉠수 있는 질문이 가장 효율적 : 이상적인 Case
    + 4번의 질문? : 1. 검정색/빨간색 카드인가 ? (색에 대한것)  2. 숫자가 짝수냐 홀수냐? 3.스페이스냐? Yes/No 4. 2냐? yes/No 
  
  + Fomula : 
    + W = 2^n (n: 정보량) (지수가 2인 이유: bit)  
    + logW = nlog2  , n = logW/log2
    + w대신에 P(확률)을 적용 : P = 1/w = 1/2^n 
    + n = -logP / log2  = -log2P (밑이 2인 로그) 
  
  + log 함수 응용
    + 확률 P1 P2... Pn 을 계속 곱하면 0으로 수렴 (컴퓨터 : underflow)
    + log 함수로 계산하면 logP1 +logP2 + ... 덧셈식으로 계산가능
  
  + Fomula2 (엔트로피 식의 확장) 
  + ex 8개의 빨간 공 / 2개의 흰 공
    + 전체 엔트로피 : -log2P 덧셈식 활용 
    + 확률의 평균값 + log2P 덧셈 
    
  + Entropy 
    + Entropy = P(x)log2 (1/P(x))
    + P(x) : 확률
    + log2(1/P(x)) : 정보량
    + 엔트로피는 항상 0보다 크다 
    + Variable 이 2개 이상일경우
      + entropy = -P1 log P1 - P2 log P2
      
  + Joint Entropy 
    + pass 
    + H(x,y) = sum of P(x,y)log(x,y) (x,y의 합계)
    
  + Conditional Entropy 
    + pass
    + H(y|x) = sum of P(x)H(y)  = P(x,y)log(y|x)
    
  + Entropy Chain Rule
    + pass
    
  + Mutual information 
    + pass 
    + 서로 얼마나 정보를 공유하는가? 
    
  + Language model and entropy
    + P(x) : 
    + q(x) :
  
  + KL divergence 
    + 분포의 distance가 얼마나 다른지? (KL divergence) 
      + https://hwiyong.tistory.com/408    


# 자연어 처리 

+ 1주차 강의 

  + 자연어 처리 파이프라인  
  
    + 1. Preprocessing (Sentence extraction(한 문장추출), word spacing(띄워쓰기), normalization(정규화->하나의 Form ex 3 -> 삼 )) 
      + 시스템 전처리 , Input 오류 보정 ,  
      + 문서 -> 단락 -> 문장 -> 어절 (띄워쓰기 단위)-> 음절 (초성+중성+종성) (을) -> 음소 (ㅇ+ㅡ+ㄹ)  
  
    + 2. Morphology 
      + 형태소 분석 (Part of speech) (POS tagging)
      + 형태소 : 어절과 음절의 사이 ? : 한 형태소는 여러개의 음절로 이뤄짐 
      + 단어를 계속 쪼갰을때 의미(Meaning)를 가지는 단위 : 
      + ex) 엔진을 -> 엔진(명사) + 을(조사)
    
    + 3. Syntax 
      + 문법요소 분석  
    
    + 4. Semantic 
      + 의미 관계 분석 
      + 동사와의 관계를 통해서 분석 가능   
      + 단어의 의미 분석 ex 사과를 먹을순 있지만 연필은 먹을수는 없지 
    
    + 5. Discourse
      +  담화분석
      +  1문장의 의미 + 앞에 나온 문장에 따라서 의미가 달라질 경우
      +  밥 먹었니? 네 
      +  밥 안먹었니? 네
      +  네의 의미가 앞문장에 따라 계속 달라짐. 
    
    + 6. NER - natural language processing  
      + 고유명사를 어떤식으로 추출할까요 ?
      + PLO (사망 지역 조직명) 
      + 스티브잡스(사람)이 설립한 애플(회사)와 가까운 맥도날드 캘리포니아(지역)지점 에서 애플(사과)파이 먹기
        
     
+ 2주차 강의 (9/6)
  
  + 자연어처리 파이프라인
    + 형태소 분석, 구문 분석, 의미 분석, 담화분석 (NLU)
    + 문장생성  (NLG)
  
  + 모호성 존재 
    + 품사 / 단어의 의미가 변동성 가짐 (문맥 , common sense의 중요성 가짐) 
  
  + 발전의 단계?
    + Rule based Model  -> 분기문 Model  -> 확률 Model (베이지안 Classifier) / Hidden Markov /  -> Deep learning     
  
  + NLP의 2가지 단계 
    + Natural Ranguage Understanding(NLU) + Generation(NLG)

  + 자연어처리 머신러닝 기법?
    + Classification : N21 (MLP SVM model ,CNN Model)
    + Sequential Labeling : N2N (ex) POS tagging , CRF Model , RNN Model ) 
    + Seq to Seq : N2M (Generation model) 
    + N2 structure (의존관계, 트리?) :  


# 기계 학습

+ 1주차 강의 
  
  + K Nearest neighbors  
  
    + classification
      + 데이터가 주어졌을때 어떤 class에 속해있나? 
      + 가장 가까운 친구 찾기 나는 누구인가 - 유유상종 알고리즘 
      + K 에 따라 label이 변할수 있음   
    
    + Regression
      + 출력이 class가 아니라 실수값으로 나옴
      + X 입력축을 기준으로 K개 가까운 X 찾고 , 그 X들의 Y 값의 평균으로 출력     
    
    + variation
      + 단순하게 하지말고 거리까지 고려할 필요가 있지 않을까?  (Not just counting)
      + 거리에 따른 가중치 W(x) 적용 , 함수 디자인 
        + 1.  W(x) 함수 디자인 , 거리반비례 : 1/distance(x1,x2)
        + 2.  Exponential 함수 이용   
        + 3.  커널 리그레션 : 가우시안 분포 적용 (?!)
        + https://data-science-hi.tistory.com/64
         
    + Distance Measure
      + kNN 의 거리를 어떤식으로 정의? 
        + 공간정의에 따라 달라짐
        + 유클리안 거리 : 공간의 span에 따라 거리가 달라질수있음
        + 다른 거리 측정법? : 코사인 유사도 , 피어슨 correlation .., etc 
        + https://dive-into-ds.tistory.com/46
         

+ 2주차 강의  
  
  + Linear regression
    + W vector (w0,w1,w2....wn) 찾기   
    + X vector (X1,X2,......Xn)
    + loss function : MSE(Mean Squared Error)
  
  + how to ? : 모든 W에 대해서 편미분 진행 (variable와 같은 갯수 equations 생성)   
  + solve ? : 식을 전개한것을 matrix 에 적용 w = A-1*B 적용
  
  + w = (XtX) -1 * (XtY)  (X transpose() )
  
    + linear regression matrix form (단순선형회귀분석 / 정규방정식)
      + https://hyeshinoh.github.io/2018/11/21/math-lin_alg-017/
      + https://jangpiano-science.tistory.com/111
      +  https://mazdah.tistory.com/831 
      +  https://omicro03.medium.com/linear-regression-63164cd2e51     
    
    + kernel regression matrix (일반화) 
      + https://analysisbugs.tistory.com/163   
  
  + Another solving : Gradient descendant , Maximum likelyhood 
  
  + Model analysis: 
    + 1. Overfitting vs Generalization   
    + 2. evaluation : Training set and Test set 
    + 3. Validation set : K fold Validation + ETC  
      + https://deep-learning-study.tistory.com/623

+ 3주차 강의 
  + Minimizes the error (Gradient Descendant) vs maximizes the probability (Maximum likelyhood)
  
  + MLE : 
    +  Linear 모델을 가지고 데이터 D 가 가장 fit (maximize) 확률로 접근 
    +  weight 와 Standard Deviation 이 주어질때 데이터(D)의 X,Y 좌표 : P(x,y|w,s)
        + Noise 가 정규분포를 띈다고 가정?   
        + Y와 X는 확률 곱법칙으로 분리가능
        + X 와 w,s 는 독립이다 (Conditional proabability 의 chain rule 적용 시)  
        + 최종적으로 loss 함수형태 유도 가능 
  
  + Logisitic regression 
    + Sigmoid fuction : 나오는 값 0 ~ 1 사이 (확률과 비슷) 
    + Soft version of linear classifier (Using Exponential Fuction)  
      + 모 아니면 도 (0/1)같은 형태가 아니라 soft 한 형태의 classifier 구현가능
      +      

  
  + Naïve Bayesian
    
    + Naïve Bayesian Classifier
      + Probability with the strongest assumption on independence 
    + Formal Description of NBC
      + 
    
+ 4주차 강의 
  
  + Constrained(제한적조건) Optimization (for SVM)
    + 주어진 데이터 :   
   
# 컴퓨터 비전

+ Computer Vision: Algorithms and Applications, 1st ed.
  + https://szeliski.org/Book/1stEdition.htm
  + http://www.cse.psu.edu/~rtc12/CSE486/

+ 1주차 강의 : 강의소개 
  
+ 2주차 강의(9/7) : Geometric camera model  (Plane to Plane Mappings)
  
  + 좌표계
    + 동형좌표계  
      + https://darkpgmr.tistory.com/77 
   
    + Image transformation
      + 이미지 변환시 자유도 계산 (DOF) 
      + https://elecs.tistory.com/151
  
  + Geometric camera model 
    
    + 외부세계 (3D) - > 카메라 -> 사진 (2D) : 
      + 사영기하학 
      + 소실포인트 : 큐브의 각도 소실 / 길이 / 평행선 소실 ex vanishinh point (철로사진 예시 )
      + 유지포인트 : 큐브의 선들의 교차점 유지  
    
    + Pinhole camera 
      + 빛에 의한 객체의 모든 정보량을 필름에 감광 X : 한 포인트만 뚫려있는 배리어(조리개)를 앞에 둔다 
      + 핀홀이크다 : 정보량 겹침(blur) / 핀홀이 너무 작다 : 정보량 너무 작음 (blur) + 회절의 문제점 발생
      
      + Pinhole Camera Matrix model : 3*4 matrix
        + https://staff.fnwi.uva.nl/r.vandenboomgaard/IPCV20162017/LectureNotes/CV/PinholeCamera/PinholeCamera.html 
        + 1. 렌즈의 공식 : (x,y,z) -> (f * x/z , f * y/z) 
        + 2. 중심점 보정 : (이미지 and detector) : Ox, Oy 값 추가로 넣어줌
        + 3. Rotation , Translation 보정
        + 4. 추가 자유도 : 카메라가 정사각형이 아닐수도 있음 (자유도 2 추가) + Skew 된 카메라일경우? (자유도 1추가) 
                 
    + Lens camera
      + 반사되는 여러가지 빛들을 효율적으로 모아서 전달 
      + 초점 : Focal point (빛이 모이는 지점) (빛의 One point (볼록렌즈 생각)) 
      + 장점 : 빛이 잘 모여질때 (초점) 잘 맞을때  (focal point: 초점) 
      + 단점 : 제대로 초점이 안 맞을때 
      + 렌즈의 모양을 바꿔가면서 진행 (눈과 같은 원리)  
      + 렌즈의 공식 : 
        + Object 크기 : y
        + Object 크기 (렌즈상) : y'  
        + distance (D): 사물 - 렌즈거리 
        + focal point (f) : 렌즈 - 초점 거리 
        + focal distance (D') : 초점거리 - 필름(Detector) 거리 
        + 1/D + 1/D' = 1/f (기하학을 이용하여 유도가능)  
      
     + Depth of Field : 피사계 심도 
        + D 가 커질수록 : D' 과 f 는 같아짐 
        + 어떤식으로 컨트롤이 가능 ? : 조리개 사이즈로 컨트롤 가능. 
        + 피사계 심도가 얇다 = 배경과 피사체 분리가능 (배경 흐릿) 
        + https://mgun.tistory.com/1388
        
      + Field of View 
        + Fov = tan-1(d/2f) 
        + f가 길어질수록 시야각은 좁아진다 
      
      + Focal length vs Viewpoint       
        + 멀면 멀수록 배경을 crop 시키는 효과 ? 
        + https://www.sony.ca/en/electronics/focal-length-angle-of-view-perspective
      

+ 3주차 강의 (9/13)
+ What is image? 
  
  + how to reduce noise? / how to find useful feature ?
  
  + Image warping : 화소의 위치 변경 / Image filtering : 화소의 값 변경 
  
    + point operation  : 변화를 주는 함수 / 픽셀 자체에만 적용 
    + Filter operation : 하나의 화소뿐만 아니라 주변에 비해 얼만큼 조절할것인가? : Neighborhood of each pixel    
  
  + Linear shift Invariant Image filtering
    
    + 하나의 이미지에 Kernel & Stride 동일하게 적용  / 어떤 kernel 을 만들어야하는가? 
      
      + Square mean filter : 가중치의 합의 평균으로 나누어줌 (blur 처리 효과)
        + Same padding  : (for 같은영상크기 : Ps = (kernels-1) /2  : 커널 사이즈3일경우 : 1패딩  
        + Valid padding : Fs(6*6 이미지) -Ps(3*3 커널사이즈) + 1 = 6-3+1 = 4  
        + 필터 조합해도 선형변환이므로 따로 연산 가능
        + shift Invariant : https://en.wikipedia.org/wiki/Shift-invariant_system
      
      + Mask 사이즈의 크기는 어떤식으로 정하나?
        + 마스크의 크기가 클수록 더 Blur : 주변을 더 고려 
        + more expensive to compute : 연산량 up
        + bigger noise spread        
      
      + Mask 원형으로 구성 가능 : (to eliminate edge effect) 
    
  + Gausian Filter 
    
    + for 격자 형태를 띄는것을 방지 : 실제로 원형의 필터 적용 (격자형태 부드럽게) 
    + 표준편차 : 곡선 거리를 조절 (표준편차가 커질수록 주변활용 폭이 넓어짐) / 필터를 키울수록(표준편차가 클수록) blur 효과  
      
      + 특징 : 회전 불변 (rotation symmetry)
      + Separability : 필터분리가능(horizontal / vertical) / 선형연산 가능 (연산량 k^2 -> 2k 연산) 
      + nxn matrix -> n^2 를 n 으로 처리가능 
  
  + Non linear filter 
  
    + Salt and pepper noise : black and white pixel
      + MEDIAN : Nonlinear 방법 이용 
        + https://preventionyun.tistory.com/33
        + 픽셀의 값 sorting (중간값) 계산 / 적용 : 대신에 이미지의 디테일이 떨어짐 / 연산량 증가 
        + Mean filter 에 비해 튀는 noise 에 강한 장점 
    + Impulse Noise  :  random peak noise pixel
    + Gaussian Noise :  


  + Convolution Filtering
  
    + Convolution Filtering and Cross-correlation ?
      + 필터가 대칭(Symmetry) 을 가지면 둘은 똑같음 (1~9 vs 9~1)   
      + https://oliveslinuxkernel.tistory.com/34
      + https://tensorflow.blog/2017/12/21/convolution-vs-cross-correlation/
      
      + Edge detecting
        + Edge 미분 값 표현 - > 이산형 표현 f(x+1) -f(x-1) / 2  
        + Convolution 적용 -1 0 1  
        + 예시 1 : Prewitt  : 스무딩 + 엣지 디텍팅 
        + 예시 2 : Sobel : 가중치 추가 (엣지를 도드라지게 (-1 0 1)*2 ) 
        + 예시 3 : DoG 가우시안 필터 미분 ? : 가우시안 연산값의 표준편차를 서로 다르게 한 후에 차이를 이용하는 것 
          + https://m.blog.naver.com/aik1919/140068508527
          + Canny Detecting 에 이용 :
        + 예시 4 : DoG(1차 가우시안) vs LoG (2차 가우시안)     
          + http://www.cse.psu.edu/~rtc12/CSE486/lecture11.pdf

+ 4주차 강의 (9/20)

+ Down-sampling 시 고해상도 -? 픽셀화 (So pielated) (해상도 떨어짐)   
  + Anti - aliasing 필요   
  
+ Sampling 
  + 샘플링 오류 (아날로그(연속적) - 디지털(이산적) 신호) 
  + 샘플링 주기를 얼마나 설정함에 따라서 다시 복원 시 문제가 될수 있음
  + Aliasing 발생 (sampling rate is not high enough to capture)
  + 대표적 예시 : Wagon - Wheel effect : 초당 프레임과 헬리콥터 날개 회전수가 일치 

+ Aliasing  :
  +  Anti-aliasing 방지  
    + 1. Oversampling (ex Nyquist rate = minimum sampling rate)    
      + 샘플링 주기를 반으로 줄이기 위해서 샘플링 주파수를 원래 주파수의 2배로  
    + 2. Filter image / subsampling 
      + 필터를 써서 먼저 노이즈를 제거 and 다운 샘플링 (정보의 손실이 될수 있음) 
          
+ Image Pyramid :    
  + Gaussian Pyramid 
    + blur and subsampling 반복 적용 
    + 응용예시 : 큰 영상) 전체feature  작은영상) 부분 feature 커널 학습
    + 제일 위의영상을 가지고 복원은 어려움..? (blur 에서 이미 정보손실)
    + Laplacian image pyramid 의 등장 (Upsampling)

+ Laplacian image pyramid
  + https://bkshin.tistory.com/entry/OpenCV-20-%EC%9D%B4%EB%AF%B8%EC%A7%80-%ED%94%BC%EB%9D%BC%EB%AF%B8%EB%93%9CImage-Pyramids
  + reconstruction 을 위해 필요한 정보는?
  + blur and subsampling 한 영상과의 subtraction 을 이용하여 edge 형태의 이미지 획득가능
  + Multiscale decomposition 
    + http://www.andrew.cmu.edu/course/15-440-s13/applications/labs/lab4/old/paper2.pdf
    + http://www.jcomputers.us/vol6/jcp0612-07.pdf  
  + blur and subsampling 한 영상과의 subtraction 을 이용하여 edge 형태의 이미지 획득가능
  + 가우시안 피라미드에서 반복 (Prediction error를 계속해서 얻는것)
  + 업스케일할때 라플라시안 피라미드 (prediction L1+L2+L3+L4 를 이용)

  
+ Interpolation : 
  + 스케일링 할때 사이 보간의 원리? 
  

+ Related Codes



# 의료 영상 

+ https://opencv.org/kornia-an-open-source-differentiable-computer-vision-library-for-pytorch/

+ Affine Matrix
  + http://www.sci.utah.edu/~acoste/uou/Image/project3/ArthurCOSTE_Project3.pdf
  + https://en.wikipedia.org/wiki/Affine_transformation
  + https://velog.io/@nayeon_p00/OpenCV-%EC%96%B4%ED%8C%8C%EC%9D%B8-%EB%B3%80%ED%99%98
  + https://gaussian37.github.io/vision-concept-geometric_transformation/
  + https://m.blog.naver.com/PostView.naver?isHttpsRedirect=true&blogId=samsjang&logNo=220504966397    
  + https://opencv-python.readthedocs.io/en/latest/doc/10.imageTransformation/imageTransformation.html

+ Image Agumentation
  + https://analysisbugs.tistory.com/266?category=972884  




# 북마크

+ https://www.philgineer.com/2020/10/awesome-machine-learning.html
+ https://wonjun.oopy.io/70a8f11f-5728-45e8-a1a5-e6303bf56767
+ https://bbongcol.github.io/deep-learning-bookmarks/
+ https://ddmix.blogspot.com/2015/11/favorites-bigdata-ml.html

# 책

+ https://d2l.ai/index.html 

+ https://shb.skku.edu/bigs/menu3/sub01.jsp#l1
